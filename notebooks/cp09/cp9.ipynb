{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #9 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un **generador de lexer**. Nos apoyaremos en el intérprete de expresiones regulares que implementamos en la clase anterior.\n",
    "\n",
    "Por cuestión de comodidad, en esta clase usaremos una versión de autómata basada en referencias entre estados.\n",
    "La clase `State`, que usaremos para modelar los estados, se encuentra en `cmp.automata`. Pasemos a importala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.automata import State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El autómata resulta de la interconexión de estados. Cualquier estado puede usarse como raíz del autómata, pero en función del estado que se escoja será el lenguaje reconocido. Un estado se contruye a partir de un objeto, que se usará para representarlo. Este puede ser un entero, o string, pero también pueden ser otros estados a su vez. Además, es necesario especificar si el estado es final o no. Por defecto se asume que **no** es final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Estado 0 -------------\n",
      "Identificador: 0\n",
      "Es final: False\n",
      "Transiciones: {'a': [0], 'b': [1]}\n",
      "Epsilon transiciones: set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Estado 1 -------------\n",
      "Identificador: 1\n",
      "Es final: True\n",
      "Transiciones: {'a': [0], 'b': [1]}\n",
      "Epsilon transiciones: set()\n"
     ]
    }
   ],
   "source": [
    "a = State(0)\n",
    "b = State(1, True)\n",
    "a.add_transition('a', a)\n",
    "a.add_transition('b', b)\n",
    "b.add_transition('a', a)\n",
    "b.add_transition('b', b)\n",
    "\n",
    "display(a)\n",
    "\n",
    "print('----------- Estado 0 -------------')\n",
    "print('Identificador:', a.state)\n",
    "print('Es final:', a.final)\n",
    "print('Transiciones:', a.transitions)\n",
    "print('Epsilon transiciones:', a.epsilon_transitions)\n",
    "\n",
    "display(b)\n",
    "\n",
    "print('----------- Estado 1 -------------')\n",
    "print('Identificador:', b.state)\n",
    "print('Es final:', b.final)\n",
    "print('Transiciones:', b.transitions)\n",
    "print('Epsilon transiciones:', b.epsilon_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las **transiciones** entre estados se añaden a través del método `<origin>.add_transition(symbol, end)`. En el caso de las **epsilon transiciones**, se usa `<origin>.add_epsilon_transition(end)`. Las transiciones entre estados son potencialmente no deterministas. Dependerá del autómata construido si se comporta de dicha forma o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = State(2)\n",
    "c.add_epsilon_transition(a)\n",
    "c.add_epsilon_transition(b)\n",
    "\n",
    "display(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se provee un mecanismo de conversión de autómata no determinista a determinista. Nótese que la transformación es solo sobre la estructura del autómata subyacente (se sigue usando la misma clase `State`). Los nodos del autómata original pasan ha formar parte del identificador de los nuevos estados: al consultar el campo `state`, se obtiene una tupla con los posibles estados en los que podría estar el autómata original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(c.to_deterministic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `State` brinda el método `from_nfa` por compatibilidad con la API de autómatas que estuvimos usando hasta la clase anterior. Este método construye una instancia de `State` a partir de una instancia de `NFA` (o por tanto de `DFA`). Se puede incluir un segundo argumento al llamado del método `from_nfa` si se desea obtener, además del estado inicial del autómata resultante, el resto de los estados que fueron creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (DFA):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.DFA at 0x7426901fd090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copia (State):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cmp.tools.automata import DFA\n",
    "\n",
    "automaton = DFA(states=3, finals=[2], transitions={\n",
    "    (0, 'a'): 0,\n",
    "    (0, 'b'): 1,\n",
    "    (1, 'a'): 2,\n",
    "    (1, 'b'): 1,\n",
    "    (2, 'a'): 0,\n",
    "    (2, 'b'): 1,\n",
    "})\n",
    "print('Original (DFA):')\n",
    "display(automaton)\n",
    "\n",
    "automaton = State.from_nfa(automaton)\n",
    "print('Copia (State):')\n",
    "display(automaton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se provee un mecanismo de reconocimiento de cadenas (independientemente de si el autómata subyacente es determinista o no). Este es accesible a través del método `<state>.recognize(string)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert automaton.recognize('ba')\n",
    "assert automaton.recognize('aababbaba')\n",
    "\n",
    "assert not automaton.recognize('')\n",
    "assert not automaton.recognize('aabaa')\n",
    "assert not automaton.recognize('aababb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de Lexer\n",
    "\n",
    "Como estudiamos en conferencia, el generador de lexer se basa en un conjunto de expresiones regulares. Cada una de ellas está asociada a un tipo de token. El lexer termina siendo un autómata finito determinista con ciertas peculiaridades:\n",
    "- Resulta de transformar el autómata unión entre todas las expresiones regulares del lenguaje, y convertirlo a determinista.\n",
    "- Cada estado final almacena los tipos de tokens que se reconocen al alcanzarlo. Se establece una prioridad entre ellos para poder desambiaguar.\n",
    "- Para tokenizar, la cadena de entrada viaja repetidas veces por el autómata.\n",
    "    - Cada vez, se comienza por el estado inicial, pero se continúa a partir de la última sección de la cadena que fue reconocida.\n",
    "    - Cuando el autómata se \"traba\" durante el reconocimiento de una cadena, se reporta la ocurrencia de un token. Su lexema está formado por la concatenación de los símbolos que fueron consumidos desde el inicio y hasta pasar por el último estado final antes de trabarse. Su tipo de token es el de mayor relevancia entre los anotados en el estado final.\n",
    "    - Al finalizar de consumir toda la cadena, se reporta el token de fin de cadena.\n",
    "\n",
    "### Expresiones regulares\n",
    "\n",
    "El engine de expresiones regulares que implementamos en la clase anterior está disponible en `cmp.tool.regex`. La clase `Regex` se instancia a partir de un string que representa la expresión regular. Cada instancia posee un campo `automaton` que permite acceder al `DFA` que reconoce el lenguaje denotado por dicha expresión regular. Las instancias de `Regex` son invocables, recibiendo como único parámentro un string, y devuelve si el string pertenece o no al lenguaje denotado por la expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.regex import Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "\n",
    "Implementemos el generador de lexer. El lexer se construirá a partir de la tabla de expresiones regulares (una lista de tuplas con la forma: `(<token_type>, <regex_str>)`). Esta tabla se recibe como el parámetro `table` en el contructor de la clase `Lexer`. La prioridad/relevancia de cada tipo de token está marcada por el índice que ocupa en la tabla. Los tipos de tokens cuyas expresiones regulares estén registradas más cerca del inicio de la tabla tiene más prioridad.\n",
    "- **_build_regexs:** devuelve un lista con los autómatas (instancias de `State`) de cada expresión regular. Los estados finales de los respectivos autómatas deben marcarse (campo `tag`) con la prioridad y tipo de token según la expresión regular que lo originó.\n",
    "- **_build_automaton:** devuelve la versión determinista del autómata que reconoce los tokens del lenguaje.\n",
    "- **_walk:** Devuelve el último estado final visitado, y lexema consumido, durante el reconocimiento del string que se recibe como entrada.\n",
    "- **_tokenize:** Devuelve tuplas de la forma `(lex, token_type)` que resultan de tokenizar la entrada. Debe manejar el caso en que la entrada no puede ser tokenizada completamente (se detecta cuando en una iteración la cadena no avanzó)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.utils import Token\n",
    "\n",
    "class Lexer:\n",
    "  def __init__(self, table, eof):\n",
    "    self.eof = eof\n",
    "    self.regexs = self._build_regexs(table)\n",
    "    self.automaton = self._build_automaton()\n",
    "    \n",
    "  def _build_regexs(self, table):\n",
    "    regexs = []\n",
    "    for n, (token_type, regex) in enumerate(table):\n",
    "      automaton = Regex(regex).automaton\n",
    "      automaton = State.from_nfa(automaton)\n",
    "      for state in automaton:\n",
    "        if state.final:\n",
    "          state.tag = (token_type,n)\n",
    "      regexs.append(automaton)\n",
    "    return regexs\n",
    "    \n",
    "  def _build_automaton(self):\n",
    "    start = State('start')\n",
    "    for automaton in self.regexs:\n",
    "      start.add_epsilon_transition(automaton) \n",
    "    return start.to_deterministic()    \n",
    "        \n",
    "  def _walk(self, string):\n",
    "    state = self.automaton\n",
    "    final = state if state.final else None\n",
    "    final_lex = lex = ''\n",
    "        \n",
    "    for symbol in string:       \n",
    "      try:\n",
    "        state = state[symbol][0] \n",
    "        lex = lex + symbol\n",
    "      except TypeError:\n",
    "        break  \n",
    "\n",
    "    final = state                    \n",
    "    final.tag = (None, float('inf'))\n",
    "    for state in final.state:\n",
    "      if state.final and state.tag[1] < final.tag[1]:\n",
    "        final.tag = state.tag\n",
    "    final_lex = lex\n",
    "\n",
    "    return final, final_lex\n",
    "    \n",
    "  def _tokenize(self, text: str):\n",
    "    remaining_text = text\n",
    "    while True:\n",
    "      final_state, final_lex = self._walk(remaining_text)\n",
    "\n",
    "      if final_lex == '':\n",
    "        yield text.rsplit(remaining_text)[0], final_state.tag[0]\n",
    "        return\n",
    "            \n",
    "      yield final_lex, final_state.tag[0] \n",
    "      remaining_text = remaining_text.replace(final_lex,'',1)\n",
    "      if remaining_text == '':\n",
    "        break\n",
    "        \n",
    "    yield '$', self.eof\n",
    "    \n",
    "  def __call__(self, text):\n",
    "    return [ Token(lex, ttype) for lex, ttype in self._tokenize(text) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos un lexer con algunas expresiones regulares solo para comprobar la validez de la implementación. Usaremos `str` para marcar los tipos de tokens, pero recuerde que realmente se usan los símbolos de la gramática ya que son los símbolos con los que trabaja el parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero digits: 1|2|3|4|5|6|7|8|9\n",
      "Letters: a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z\n",
      "\n",
      ">>> Tokenizando: \"5465 for 45foreach fore\"\n",
      "[num: 5465, space:  , for: for, space:  , num: 45, foreach: foreach, space:  , id: fore, eof: $]\n",
      "\n",
      ">>> Tokenizando: \"4forense forforeach for4foreach foreach 4for\"\n",
      "[num: 4, id: forense, space:  , id: forforeach, space:  , id: for4foreach, space:  , foreach: foreach, space:  , num: 4, for: for, eof: $]\n"
     ]
    }
   ],
   "source": [
    "nonzero_digits = '|'.join(str(n) for n in range(1,10))\n",
    "letters = '|'.join(chr(n) for n in range(ord('a'),ord('z')+1))\n",
    "\n",
    "print('Non-zero digits:', nonzero_digits)\n",
    "print('Letters:', letters)\n",
    "\n",
    "lexer = Lexer([\n",
    "    ('num', f'({nonzero_digits})(0|{nonzero_digits})*'),\n",
    "    ('for' , 'for'),\n",
    "    ('foreach' , 'foreach'),\n",
    "    ('space', '  *'),\n",
    "    ('id', f'({letters})({letters}|0|{nonzero_digits})*')\n",
    "], 'eof')\n",
    "\n",
    "text = '5465 for 45foreach fore'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)\n",
    "assert [t.token_type for t in tokens] == ['num', 'space', 'for', 'space', 'num', 'foreach', 'space', 'id', 'eof']\n",
    "assert [t.lex for t in tokens] == ['5465', ' ', 'for', ' ', '45', 'foreach', ' ', 'fore', '$']\n",
    "\n",
    "text = '4forense forforeach for4foreach foreach 4for'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)\n",
    "assert [t.token_type for t in tokens] == ['num', 'id', 'space', 'id', 'space', 'id', 'space', 'foreach', 'space', 'num', 'for', 'eof']\n",
    "assert [t.lex for t in tokens] == ['4', 'forense', ' ', 'forforeach', ' ', 'for4foreach', ' ', 'foreach', ' ', '4', 'for', '$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Genere un lexer para el lenguaje de expresiones aritméticas que estuvimos trabajando a inicios del curso.\n",
    "- Implemente un algoritmo para eliminar los estados muertos de un autómata. Recordemos que esto es conveniente puesto que de esta forma el autómata del lexer detecta los tokens más de prisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.automata import NFA, DFA, nfa_to_dfa\n",
    "from cmp.tools.automata import automata_union, automata_concatenation, automata_closure, automata_minimization\n",
    "\n",
    "class Node:\n",
    "  def evaluate(self):\n",
    "    raise NotImplementedError()        \n",
    "class AtomicNode(Node):\n",
    "  def __init__(self, lex):\n",
    "    self.lex = lex\n",
    "class UnaryNode(Node):\n",
    "  def __init__(self, node):\n",
    "    self.node = node        \n",
    "  def evaluate(self):\n",
    "    value = self.node.evaluate() \n",
    "    return self.operate(value)    \n",
    "  @staticmethod\n",
    "  def operate(value):\n",
    "    raise NotImplementedError()        \n",
    "class BinaryNode(Node):\n",
    "  def __init__(self, left, right):\n",
    "    self.left = left\n",
    "    self.right = right        \n",
    "  def evaluate(self):\n",
    "    lvalue = self.left.evaluate() \n",
    "    rvalue = self.right.evaluate()\n",
    "    return self.operate(lvalue, rvalue)    \n",
    "  @staticmethod\n",
    "  def operate(lvalue, rvalue):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "EPSILON = 'ε'\n",
    "class EpsilonNode(AtomicNode):\n",
    "  def evaluate(self):\n",
    "    return NFA(states=2, finals=[1], transitions={(0, ''): [1]})\n",
    "class SymbolNode(AtomicNode):\n",
    "  def evaluate(self):\n",
    "    s = self.lex\n",
    "    return NFA(states=2, finals=[1], transitions={(0, s): [1]})\n",
    "class ClosureNode(UnaryNode):\n",
    "  @staticmethod\n",
    "  def operate(value):\n",
    "    return automata_closure(value)\n",
    "class UnionNode(BinaryNode):\n",
    "  @staticmethod\n",
    "  def operate(lvalue, rvalue):\n",
    "    return automata_union(lvalue,rvalue)\n",
    "class ConcatNode(BinaryNode):\n",
    "  @staticmethod\n",
    "  def operate(lvalue, rvalue):\n",
    "    return automata_concatenation(lvalue,rvalue)\n",
    "\n",
    "from cmp.pycompiler import Grammar\n",
    "\n",
    "G = Grammar()\n",
    "\n",
    "E = G.NonTerminal('E', True)\n",
    "T, F, A, X, Y, Z = G.NonTerminals('T F A X Y Z')\n",
    "pipe, star, opar, cpar, symbol, epsilon = G.Terminals('| * ( ) symbol ε')\n",
    "\n",
    "E %= T + X, lambda h,s: s[2], None, lambda h,s: s[1]\n",
    "\n",
    "X %= pipe + T + X, lambda h,s: s[3], None, None, lambda h,s: UnionNode(h[0],s[2])                            \n",
    "X %= G.Epsilon, lambda h,s: h[0]\n",
    "\n",
    "T %= F + Y, lambda h,s: s[2], None, lambda h,s: s[1]  \n",
    "\n",
    "Y %= F + Y, lambda h,s: s[2], None, lambda h,s: ConcatNode(h[0],s[1])                            \n",
    "Y %= G.Epsilon, lambda h,s: h[0] \n",
    "\n",
    "F %= A + Z, lambda h,s: s[2], None, lambda h,s: s[1]\n",
    "\n",
    "Z %= star, lambda h,s: ClosureNode(h[0]), None\n",
    "Z %= G.Epsilon, lambda h,s: h[0]\n",
    "\n",
    "A %= symbol, lambda h,s: SymbolNode(s[1]), None  \n",
    "A %= epsilon, lambda h,s: EpsilonNode(s[1]), None                                                  \n",
    "A %= opar + E + cpar, lambda h,s: s[2], None, None, None \n",
    "\n",
    "\n",
    "def regex_tokenizer(text, G, skip_whitespaces=True):\n",
    "  tokens = []    \n",
    "  fixed_tokens = { lex: Token(lex, G[lex]) for lex in '| * ( ) ε'.split() }\n",
    "  special_char = False\n",
    "  for char in text:\n",
    "    if skip_whitespaces and char.isspace():\n",
    "      continue\n",
    "    elif special_char:\n",
    "      token = Token(char, G['symbol'])\n",
    "      special_char = False            \n",
    "    elif char == '\\\\':\n",
    "      special_char = True\n",
    "      continue \n",
    "    else:\n",
    "      try:\n",
    "        token = fixed_tokens[char]\n",
    "      except:\n",
    "        token = Token(char, G['symbol'])\n",
    "    tokens.append(token)        \n",
    "  tokens.append(Token('$', G.EOF))\n",
    "  return tokens\n",
    "\n",
    "\n",
    "from cmp.tools.parsing import metodo_predictivo_no_recursivo\n",
    "from cmp.tools.evaluation import evaluate_parse\n",
    "\n",
    "parser = metodo_predictivo_no_recursivo(G)\n",
    "\n",
    "def regex_automaton(regex):\n",
    "  regex_tokens = regex_tokenizer(regex,G)\n",
    "  regex_parser = parser(regex_tokens)\n",
    "  regex_ast = evaluate_parse(regex_parser,regex_tokens)\n",
    "  regex_nfa = regex_ast.evaluate()\n",
    "  return automata_minimization(nfa_to_dfa(regex_nfa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "  \n",
    "  def __init__(self, table, eof):\n",
    "    self.eof = eof\n",
    "    self.regexs = self._build_regexs(table)\n",
    "    self.automaton = self._build_automaton()  \n",
    "\n",
    "  def _build_regexs(self, table):\n",
    "    regexs = []\n",
    "    for n, (token_type, regex) in enumerate(table):            \n",
    "      automaton = regex_automaton(regex)\n",
    "      automaton = State.from_nfa(automaton)   \n",
    "      for state in automaton:\n",
    "        if state.final:\n",
    "          state.tag = (token_type,n)\n",
    "      regexs.append(automaton)\n",
    "    return regexs   \n",
    "\n",
    "  def _build_automaton(self):\n",
    "    start = State('start')\n",
    "    for automaton in self.regexs:\n",
    "      start.add_epsilon_transition(automaton) \n",
    "    return start.to_deterministic()     \n",
    "\n",
    "  def _walk(self, string):\n",
    "    state = self.automaton\n",
    "    final = state if state.final else None\n",
    "    final_lex = lex = ''        \n",
    "    for symbol in string:\n",
    "      try:\n",
    "        state = state[symbol][0] \n",
    "        lex = lex + symbol\n",
    "      except TypeError:\n",
    "        break       \n",
    "    final = state                    \n",
    "    final.tag = (None, float('inf'))\n",
    "    for state in final.state:\n",
    "      if state.final and state.tag[1] < final.tag[1]:\n",
    "        final.tag = state.tag\n",
    "    final_lex = lex          \n",
    "    return final, final_lex    \n",
    "\n",
    "  def _tokenize(self, text: str, skip_whitespaces=True):\n",
    "    remaining_text = text\n",
    "    while True:\n",
    "      if skip_whitespaces and remaining_text[0].isspace():\n",
    "        remaining_text = remaining_text[1:]\n",
    "        continue                \n",
    "      final_state, final_lex = self._walk(remaining_text)\n",
    "      if final_lex == '':\n",
    "        yield text.rsplit(remaining_text)[0], final_state.tag[0]\n",
    "        return            \n",
    "      yield final_lex, final_state.tag[0] \n",
    "      remaining_text = remaining_text.replace(final_lex,'',1)\n",
    "      if remaining_text == '':\n",
    "        break        \n",
    "    yield '$', self.eof   \n",
    "\n",
    "  def __call__(self, text, skip_whitespaces=True):\n",
    "    return [ Token(lex, ttype) for lex, ttype in self._tokenize(text, skip_whitespaces) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Tokenizando: \"let    x=10,y=222 in (332823948*xiom304230)\"\n",
      "[let: let, id: x, equals: =, num: 10, comma: ,, id: y, equals: =, num: 222, in: in, opar: (, num: 332823948, star: *, id: xiom304230, cpar: ), eof: $]\n"
     ]
    }
   ],
   "source": [
    "lexer = Lexer([\n",
    "  ('num', f'({nonzero_digits})(0|{nonzero_digits})*'),\n",
    "  ('plus', '\\\\+'),\n",
    "  ('minus', '\\\\-'),\n",
    "  ('star', '\\\\*'),\n",
    "  ('div', '\\\\/'),\n",
    "  ('pow', '\\\\^'),\n",
    "  ('opar', '\\\\('),\n",
    "  ('cpar', '\\\\)'),\n",
    "  ('comma', ','),\n",
    "  ('equals', '='),\n",
    "  ('let', 'let'),\n",
    "  ('in', 'in'),       \n",
    "  ('id', f'({letters})({letters}|0|{nonzero_digits})*')\n",
    "], 'eof')\n",
    "\n",
    "text = 'let    x=10,y=222 in (332823948*xiom304230)'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
